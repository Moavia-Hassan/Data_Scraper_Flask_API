{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medium Article Scraper\n",
    "\n",
    "This notebook scrapes Medium articles to extract:\n",
    "- Title\n",
    "- Author and author URL\n",
    "- Article content\n",
    "- Claps count\n",
    "- Reading time\n",
    "- Publication\n",
    "- Date\n",
    "- Image sources\n",
    "\n",
    "The data will be saved in a CSV file with 500 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text content\"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    return ' '.join(text.strip().split())\n",
    "\n",
    "def extract_article_info(soup, url):\n",
    "    \"\"\"Extract all required information from a Medium article\"\"\"\n",
    "    article = {'url': url}\n",
    "    \n",
    "    # Extract title\n",
    "    try:\n",
    "        title_tag = soup.find('h1')\n",
    "        article['title'] = clean_text(title_tag.text) if title_tag else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting title: {str(e)}\")\n",
    "        article['title'] = None\n",
    "\n",
    "    # Extract subtitles (h2 tags)\n",
    "    try:\n",
    "        subtitles = soup.find_all('h2')\n",
    "        article['subtitles'] = [clean_text(h2.text) for h2 in subtitles]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting subtitles: {str(e)}\")\n",
    "        article['subtitles'] = None\n",
    "\n",
    "    # Extract author information\n",
    "    try:\n",
    "        author_tag = soup.find('a', {'rel': 'author'}) or soup.find('a', {'class': 'author'})\n",
    "        if author_tag:\n",
    "            article['author'] = clean_text(author_tag.text)\n",
    "            article['author_url'] = author_tag.get('href')\n",
    "            if not article['author_url'].startswith('http'):\n",
    "                article['author_url'] = f\"https://medium.com{article['author_url']}\"\n",
    "        else:\n",
    "            article['author'] = None\n",
    "            article['author_url'] = None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting author info: {str(e)}\")\n",
    "        article['author'] = None\n",
    "        article['author_url'] = None\n",
    "\n",
    "    # Extract claps count\n",
    "    try:\n",
    "        claps_button = soup.find('button', string=re.compile(r'[\\d.KM]+\\s*claps?', re.IGNORECASE))\n",
    "        if claps_button:\n",
    "            claps_text = claps_button.text.strip().lower()  # e.g., \"1.2K claps\" or \"50 claps\"\n",
    "            # Extract number and multiplier\n",
    "            match = re.search(r'([\\d.]+)([km]?)', claps_text)\n",
    "            if match:\n",
    "                number = float(match.group(1))  # e.g., 1.2 or 50\n",
    "                multiplier = match.group(2)     # e.g., \"k\", \"m\", or empty\n",
    "                if multiplier == 'k':\n",
    "                    article['claps'] = int(number * 1000)    # e.g., 1.2K -> 1200\n",
    "                elif multiplier == 'm':\n",
    "                    article['claps'] = int(number * 1000000) # e.g., 3M -> 3000000\n",
    "                else:\n",
    "                    article['claps'] = int(number)           # e.g., 50 -> 50\n",
    "            else:\n",
    "                article['claps'] = 0\n",
    "        else:\n",
    "            article['claps'] = 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting claps: {str(e)}\")\n",
    "        article['claps'] = 0\n",
    "\n",
    "    # Extract reading time\n",
    "    try:\n",
    "        reading_time = soup.find('span', string=re.compile(r'\\d+\\s*min read'))\n",
    "        if reading_time:\n",
    "            article['reading_time'] = int(''.join(filter(str.isdigit, reading_time.text)))\n",
    "        else:\n",
    "            article['reading_time'] = None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting reading time: {str(e)}\")\n",
    "        article['reading_time'] = None\n",
    "\n",
    "    # Extract full article content\n",
    "    try:\n",
    "        article_sections = soup.find_all(['p', 'h2', 'h3', 'blockquote'])\n",
    "        article['content'] = '\\n'.join([clean_text(section.text) for section in article_sections])\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting content: {str(e)}\")\n",
    "        article['content'] = None\n",
    "\n",
    "    # Extract image sources\n",
    "    try:\n",
    "        images = soup.find_all('img')\n",
    "        article['image_sources'] = [img.get('src') for img in images if img.get('src') \n",
    "                                   and not img.get('src').endswith(('.svg', '.gif'))]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting images: {str(e)}\")\n",
    "        article['image_sources'] = None\n",
    "\n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main Scraping Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URLs to process: 500\n",
      "\n",
      "Starting to scrape articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 50/500 [02:51<24:27,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: 50 articles processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 63/500 [03:39<22:52,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error processing https://siddarth.design/taking-back-the-control-e6c28305ce51?source=tag_archive---------62----------------------- - HTTPSConnectionPool(host='siddarth.design', port=443): Max retries exceeded with url: /taking-back-the-control-e6c28305ce51?source=tag_archive---------62----------------------- (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AE5AC86420>: Failed to resolve 'siddarth.design' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 83/500 [04:43<18:47,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error processing https://blog.polyverse.io/polyverse-weekly-breach-report-1d1601e11e3f?source=tag_archive---------82----------------------- - HTTPSConnectionPool(host='blog.polyverse.io', port=443): Max retries exceeded with url: /polyverse-weekly-breach-report-1d1601e11e3f?source=tag_archive---------82----------------------- (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AE5F55FD70>: Failed to resolve 'blog.polyverse.io' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 100/500 [05:40<22:48,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: 100 articles processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 150/500 [08:34<19:00,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: 150 articles processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 169/500 [09:37<17:30,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error processing https://blog.pixels.camp/first-keynote-speakers-9c5d778764be?source=tag_archive---------63----------------------- - HTTPSConnectionPool(host='blog.pixels.camp', port=443): Max retries exceeded with url: /first-keynote-speakers-9c5d778764be?source=tag_archive---------63----------------------- (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AE5EA48C80>: Failed to resolve 'blog.pixels.camp' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 172/500 [09:50<19:59,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error processing https://tincture.io/2019-forecast-amaras-law-996a8ef7f2c9?source=tag_archive---------66----------------------- - HTTPSConnectionPool(host='tincture.io', port=443): Max retries exceeded with url: /2019-forecast-amaras-law-996a8ef7f2c9?source=tag_archive---------66----------------------- (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AE5EA9F590>: Failed to resolve 'tincture.io' ([Errno 11002] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 200/500 [11:33<17:57,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: 200 articles processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 250/500 [14:29<14:45,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: 250 articles processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 260/500 [15:02<12:44,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error processing https://metizamagazine.com/the-best-brain-training-apps-4cde3973e27f?source=tag_archive---------154----------------------- - HTTPSConnectionPool(host='metizamagazine.com', port=443): Max retries exceeded with url: /the-best-brain-training-apps-4cde3973e27f?source=tag_archive---------154----------------------- (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1010)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 300/500 [17:05<10:20,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: 300 articles processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 310/500 [17:39<11:10,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error processing https://blog.usejournal.com/the-mythical-10x-programmer-e759a4ba2f0?source=tag_archive---------11----------------------- - HTTPSConnectionPool(host='blog.usejournal.com', port=443): Max retries exceeded with url: /the-mythical-10x-programmer-e759a4ba2f0?source=tag_archive---------11----------------------- (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AE607C60C0>: Failed to resolve 'blog.usejournal.com' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 314/500 [17:52<11:01,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error processing https://blog.gojekengineering.com/gojeks-impact-on-indonesia-s-gig-economy-990a60cd23b9?source=tag_archive---------15----------------------- - HTTPSConnectionPool(host='blog.gojekengineering.com', port=443): Max retries exceeded with url: /gojeks-impact-on-indonesia-s-gig-economy-990a60cd23b9?source=tag_archive---------15----------------------- (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE5A0311C0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 341/500 [19:19<07:47,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error processing https://blog.kstart.in/this-and-that-i-am-excited-for-2019-497e6fda4461?source=tag_archive---------42----------------------- - HTTPSConnectionPool(host='blog.kstart.in', port=443): Max retries exceeded with url: /this-and-that-i-am-excited-for-2019-497e6fda4461?source=tag_archive---------42----------------------- (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AE5FAF2A50>: Failed to resolve 'blog.kstart.in' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 350/500 [19:52<08:53,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: 350 articles processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 400/500 [22:37<04:59,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: 400 articles processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 450/500 [25:23<02:56,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: 450 articles processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 478/500 [26:59<01:09,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error processing https://blog.uplabs.com/10-years-2-startups-later-i-finally-built-what-i-always-needed-to-create-websites-94bdd0b498cf?source=tag_archive---------179----------------------- - HTTPSConnectionPool(host='blog.uplabs.com', port=443): Max retries exceeded with url: /10-years-2-startups-later-i-finally-built-what-i-always-needed-to-create-websites-94bdd0b498cf?source=tag_archive---------179----------------------- (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AE607A1010>: Failed to resolve 'blog.uplabs.com' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [28:03<00:00,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: 500 articles processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load URLs\n",
    "data = pd.read_csv('url_technology.csv')\n",
    "data.columns = ['urls']\n",
    "# Take first 500 URLs\n",
    "data = data.head(500)\n",
    "print(f\"Total URLs to process: {len(data)}\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "articles = []\n",
    "failed_urls = []\n",
    "\n",
    "# Main scraping loop with progress bar\n",
    "print(\"\\nStarting to scrape articles...\")\n",
    "for i, url in tqdm(enumerate(data['urls']), total=len(data)):\n",
    "    try:\n",
    "        # Add random delay between requests (1-3 seconds)\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "        \n",
    "        # Try to scrape the article\n",
    "        article_info = scrape_medium_article(url)\n",
    "        articles.append(article_info)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {url} - {str(e)}\")\n",
    "        failed_urls.append({'url': url, 'error': str(e)})\n",
    "    \n",
    "    # Save progress every 50 articles\n",
    "    if (i + 1) % 50 == 0:\n",
    "        df = pd.DataFrame(articles)\n",
    "        df.to_csv(f'articles_checkpoint_{i+1}.csv', index=False)\n",
    "        print(f\"\\nCheckpoint saved: {i+1} articles processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving final results...\n",
      "\n",
      "Scraping completed!\n",
      "Successfully scraped: 491 articles\n",
      "Failed URLs: 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>author_url</th>\n",
       "      <th>claps</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>image_sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://medium.com/javascript-scene/top-javasc...</td>\n",
       "      <td>Top JavaScript Frameworks and Topics to Learn ...</td>\n",
       "      <td>Eric Elliott</td>\n",
       "      <td>https://medium.com/@_ericelliott</td>\n",
       "      <td>36.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Jan 1, 2019</td>\n",
       "      <td>Top JavaScript Frameworks and Topics to Learn ...</td>\n",
       "      <td>[\"https://miro.medium.com/v2/resize:fill:88:88...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://medium.com/job-advice-for-software-eng...</td>\n",
       "      <td>What I want (and don’t want) to see on your so...</td>\n",
       "      <td>James S. Fisher</td>\n",
       "      <td>https://medium.com/@jamessfisher</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Jan 1, 2019</td>\n",
       "      <td>What I want (and don’t want) to see on your so...</td>\n",
       "      <td>[\"https://miro.medium.com/v2/resize:fill:88:88...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://itnext.io/load-testing-using-apache-jm...</td>\n",
       "      <td>Load Testing Using Apache JMeter</td>\n",
       "      <td>Mitesh</td>\n",
       "      <td>https://itnext.io/@mitesh_shamra</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Jan 1, 2019</td>\n",
       "      <td>Load Testing Using Apache JMeter Mitesh · Publ...</td>\n",
       "      <td>[\"https://miro.medium.com/v2/resize:fill:88:88...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://medium.com/s/story/black-mirror-bander...</td>\n",
       "      <td>The Illusion of Control in ‘Black Mirror: Band...</td>\n",
       "      <td>Howard Chai</td>\n",
       "      <td>https://howard-chai.medium.com</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Jan 1, 2019</td>\n",
       "      <td>The Illusion of Control in ‘Black Mirror: Band...</td>\n",
       "      <td>[\"https://miro.medium.com/v2/resize:fill:88:88...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://medium.com/fast-company/the-worst-desi...</td>\n",
       "      <td>The Worst Design Crimes of 2018</td>\n",
       "      <td>Fast Company</td>\n",
       "      <td>https://medium.com/@FastCompany</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Jan 1, 2019</td>\n",
       "      <td>The Worst Design Crimes of 2018 From tech comp...</td>\n",
       "      <td>[\"https://miro.medium.com/v2/resize:fill:88:88...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://medium.com/javascript-scene/top-javasc...   \n",
       "1  https://medium.com/job-advice-for-software-eng...   \n",
       "2  https://itnext.io/load-testing-using-apache-jm...   \n",
       "3  https://medium.com/s/story/black-mirror-bander...   \n",
       "4  https://medium.com/fast-company/the-worst-desi...   \n",
       "\n",
       "                                               title           author  \\\n",
       "0  Top JavaScript Frameworks and Topics to Learn ...     Eric Elliott   \n",
       "1  What I want (and don’t want) to see on your so...  James S. Fisher   \n",
       "2                   Load Testing Using Apache JMeter           Mitesh   \n",
       "3  The Illusion of Control in ‘Black Mirror: Band...      Howard Chai   \n",
       "4                    The Worst Design Crimes of 2018     Fast Company   \n",
       "\n",
       "                         author_url  claps  reading_time         date  \\\n",
       "0  https://medium.com/@_ericelliott   36.0          10.0  Jan 1, 2019   \n",
       "1  https://medium.com/@jamessfisher   25.0           8.0  Jan 1, 2019   \n",
       "2  https://itnext.io/@mitesh_shamra    1.0           6.0  Jan 1, 2019   \n",
       "3    https://howard-chai.medium.com   10.0           8.0  Jan 1, 2019   \n",
       "4   https://medium.com/@FastCompany   10.0           5.0  Jan 1, 2019   \n",
       "\n",
       "                                             content  \\\n",
       "0  Top JavaScript Frameworks and Topics to Learn ...   \n",
       "1  What I want (and don’t want) to see on your so...   \n",
       "2  Load Testing Using Apache JMeter Mitesh · Publ...   \n",
       "3  The Illusion of Control in ‘Black Mirror: Band...   \n",
       "4  The Worst Design Crimes of 2018 From tech comp...   \n",
       "\n",
       "                                       image_sources  \n",
       "0  [\"https://miro.medium.com/v2/resize:fill:88:88...  \n",
       "1  [\"https://miro.medium.com/v2/resize:fill:88:88...  \n",
       "2  [\"https://miro.medium.com/v2/resize:fill:88:88...  \n",
       "3  [\"https://miro.medium.com/v2/resize:fill:88:88...  \n",
       "4  [\"https://miro.medium.com/v2/resize:fill:88:88...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save final results\n",
    "print(\"\\nSaving final results...\")\n",
    "articles_df = pd.DataFrame(articles)\n",
    "\n",
    "# Convert list columns to string representation\n",
    "articles_df['image_sources'] = articles_df['image_sources'].apply(lambda x: json.dumps(x) if x else None)\n",
    "\n",
    "# Save to CSV\n",
    "articles_df.to_csv('medium_articles_full.csv', index=False)\n",
    "failed_df = pd.DataFrame(failed_urls)\n",
    "failed_df.to_csv('failed_urls.csv', index=False)\n",
    "\n",
    "print(f\"\\nScraping completed!\")\n",
    "print(f\"Successfully scraped: {len(articles)} articles\")\n",
    "print(f\"Failed URLs: {len(failed_urls)}\")\n",
    "\n",
    "# Display first few rows of the results\n",
    "articles_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
